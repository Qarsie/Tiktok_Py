{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f491a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Analyzing topic evolution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eisra\\AppData\\Local\\Temp\\ipykernel_18912\\3124031494.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['virality_score'] = df.groupby(pd.Grouper(key='create_time', freq=f'{WINDOW_DAYS}D')).apply(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 88\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Analyze topic evolution\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAnalyzing topic evolution...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m topic_evolution = \u001b[43manalyze_emerging_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# ======================\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# ENHANCED SENTIMENT ANALYSIS\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# ======================\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdetailed_sentiment_analysis\u001b[39m(text):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36manalyze_emerging_topics\u001b[39m\u001b[34m(df, window_days)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Apply LDA\u001b[39;00m\n\u001b[32m     71\u001b[39m lda = LatentDirichletAllocation(n_components=N_TOPICS, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m topic_dist = \u001b[43mlda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Get top words for trending content\u001b[39;00m\n\u001b[32m     75\u001b[39m trending_mask = window_data[\u001b[33m'\u001b[39m\u001b[33mis_trending\u001b[39m\u001b[33m'\u001b[39m] == \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eisra\\miniconda3\\envs\\tiktok_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eisra\\miniconda3\\envs\\tiktok_env\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:778\u001b[39m, in \u001b[36mLatentDirichletAllocation.fit_transform\u001b[39m\u001b[34m(self, X, y, normalize)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m, *, normalize=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    756\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    757\u001b[39m \u001b[33;03m    Fit to data, then transform it.\u001b[39;00m\n\u001b[32m    758\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    776\u001b[39m \u001b[33;03m        Transformed array.\u001b[39;00m\n\u001b[32m    777\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m.transform(X, normalize=normalize)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eisra\\miniconda3\\envs\\tiktok_env\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eisra\\miniconda3\\envs\\tiktok_env\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:674\u001b[39m, in \u001b[36mLatentDirichletAllocation.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    666\u001b[39m         \u001b[38;5;28mself\u001b[39m._em_step(\n\u001b[32m    667\u001b[39m             X[idx_slice, :],\n\u001b[32m    668\u001b[39m             total_samples=n_samples,\n\u001b[32m    669\u001b[39m             batch_update=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    670\u001b[39m             parallel=parallel,\n\u001b[32m    671\u001b[39m         )\n\u001b[32m    672\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    673\u001b[39m     \u001b[38;5;66;03m# batch update\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m674\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_em_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_update\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparallel\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[38;5;66;03m# check perplexity\u001b[39;00m\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m evaluate_every > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (i + \u001b[32m1\u001b[39m) % evaluate_every == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eisra\\miniconda3\\envs\\tiktok_env\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:523\u001b[39m, in \u001b[36mLatentDirichletAllocation._em_step\u001b[39m\u001b[34m(self, X, total_samples, batch_update, parallel)\u001b[39m\n\u001b[32m    496\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"EM update for 1 iteration.\u001b[39;00m\n\u001b[32m    497\u001b[39m \n\u001b[32m    498\u001b[39m \u001b[33;03mupdate `_component` by batch VB or online VB.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    519\u001b[39m \u001b[33;03m    Unnormalized document topic distribution.\u001b[39;00m\n\u001b[32m    520\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    522\u001b[39m \u001b[38;5;66;03m# E-step\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m _, suff_stats = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_e_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcal_sstats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_init\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparallel\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;66;03m# M-step\u001b[39;00m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_update:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eisra\\miniconda3\\envs\\tiktok_env\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:466\u001b[39m, in \u001b[36mLatentDirichletAllocation._e_step\u001b[39m\u001b[34m(self, X, cal_sstats, random_init, parallel)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m parallel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    465\u001b[39m     parallel = Parallel(n_jobs=n_jobs, verbose=\u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m.verbose - \u001b[32m1\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m results = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_update_doc_distribution\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexp_dirichlet_component_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdoc_topic_prior_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_doc_update_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean_change_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcal_sstats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx_slice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[38;5;66;03m# merge result\u001b[39;00m\n\u001b[32m    480\u001b[39m doc_topics, sstats_list = \u001b[38;5;28mzip\u001b[39m(*results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eisra\\miniconda3\\envs\\tiktok_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eisra\\miniconda3\\envs\\tiktok_env\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eisra\\miniconda3\\envs\\tiktok_env\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eisra\\miniconda3\\envs\\tiktok_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     config = {}\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eisra\\miniconda3\\envs\\tiktok_env\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:145\u001b[39m, in \u001b[36m_update_doc_distribution\u001b[39m\u001b[34m(X, exp_topic_word_distr, doc_topic_prior, max_doc_update_iter, mean_change_tol, cal_sstats, random_state)\u001b[39m\n\u001b[32m    141\u001b[39m last_d = doc_topic_d\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# The optimal phi_{dwk} is proportional to\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# exp(E[log(theta_{dk})]) * exp(E[log(beta_{dw})]).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m norm_phi = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_doc_topic_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_topic_word_d\u001b[49m\u001b[43m)\u001b[49m + eps\n\u001b[32m    147\u001b[39m doc_topic_d = exp_doc_topic_d * np.dot(cnts / norm_phi, exp_topic_word_d.T)\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# Note: adds doc_topic_prior to doc_topic_d, in-place.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# 5. Trend Identification Using NLP\n",
    "# Goal: Use NLP to analyze TikTok video descriptions and hashtags to identify emerging trends.\n",
    "# =======================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from datetime import timedelta\n",
    "from textblob import TextBlob\n",
    "\n",
    "# --- Configuration ---\n",
    "N_TOPICS = 5\n",
    "N_TOP_WORDS = 10\n",
    "N_TOP_ENTITIES = 20\n",
    "WINDOW_DAYS = 30  # Rolling window for trend analysis\n",
    "\n",
    "# ======================\n",
    "# DATA LOADING & PREPARATION\n",
    "# ======================\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv('data/processed/tiktok_processed.csv')\n",
    "df['create_time'] = pd.to_datetime(df['create_time'])\n",
    "df = df.sort_values('create_time')\n",
    "df['clean_description'] = df['clean_description'].fillna('')\n",
    "\n",
    "# Use virality score from q4 instead of engagement rate\n",
    "def calculate_virality_score(group):\n",
    "    \"\"\"Calculate virality score using correlation-based weights\"\"\"\n",
    "    corr_views_likes = group['plays'].corr(group['likes']) or 0.5\n",
    "    corr_views_comments = group['plays'].corr(group['comments']) or 0.5\n",
    "    corr_views_shares = group['plays'].corr(group['shares']) or 0.5\n",
    "    \n",
    "    return (group['plays'] + \n",
    "            (1 - corr_views_likes) * group['likes'] + \n",
    "            (1 - corr_views_comments) * group['comments'] + \n",
    "            (1 - corr_views_shares) * group['shares'])\n",
    "\n",
    "# Calculate rolling virality scores\n",
    "df['virality_score'] = df.groupby(pd.Grouper(key='create_time', freq=f'{WINDOW_DAYS}D')).apply(\n",
    "    lambda x: calculate_virality_score(x)\n",
    ").reset_index(level=0, drop=True)\n",
    "\n",
    "# Define trending threshold based on virality score\n",
    "virality_threshold = df['virality_score'].quantile(0.80)\n",
    "df['is_trending'] = (df['virality_score'] >= virality_threshold).astype(int)\n",
    "\n",
    "# ======================\n",
    "# TEMPORAL TREND ANALYSIS\n",
    "# ======================\n",
    "def analyze_emerging_topics(df, window_days=30):\n",
    "    \"\"\"Analyze topic evolution over time\"\"\"\n",
    "    results = []\n",
    "    for end_date in df['create_time'].unique():\n",
    "        start_date = end_date - pd.Timedelta(days=window_days)\n",
    "        window_data = df[(df['create_time'] >= start_date) & (df['create_time'] <= end_date)]\n",
    "        \n",
    "        if len(window_data) < 100:  # Skip if too few samples\n",
    "            continue\n",
    "            \n",
    "        # Vectorize text\n",
    "        vectorizer = CountVectorizer(max_features=1000, stop_words='english', ngram_range=(1, 2))\n",
    "        dtm = vectorizer.fit_transform(window_data['clean_description'])\n",
    "        \n",
    "        # Apply LDA\n",
    "        lda = LatentDirichletAllocation(n_components=N_TOPICS, random_state=42)\n",
    "        topic_dist = lda.fit_transform(dtm)\n",
    "        \n",
    "        # Get top words for trending content\n",
    "        trending_mask = window_data['is_trending'] == 1\n",
    "        if trending_mask.any():\n",
    "            trending_topics = topic_dist[trending_mask].mean(axis=0)\n",
    "            results.append({\n",
    "                'date': end_date,\n",
    "                'top_topics': trending_topics,\n",
    "                'vocab': vectorizer.get_feature_names_out()\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Analyze topic evolution\n",
    "print(\"\\nAnalyzing topic evolution...\")\n",
    "topic_evolution = analyze_emerging_topics(df)\n",
    "\n",
    "# ======================\n",
    "# ENHANCED SENTIMENT ANALYSIS\n",
    "# ======================\n",
    "def detailed_sentiment_analysis(text):\n",
    "    \"\"\"Perform detailed sentiment analysis\"\"\"\n",
    "    blob = TextBlob(text)\n",
    "    return {\n",
    "        'polarity': blob.sentiment.polarity,\n",
    "        'subjectivity': blob.sentiment.subjectivity,\n",
    "        'word_count': len(blob.words),\n",
    "        'has_exclamation': '!' in text,\n",
    "        'has_question': '?' in text,\n",
    "        'has_emoji': any(char in text for char in ['😀', '😂', '❤️', '🔥'])  # Add more emojis\n",
    "    }\n",
    "\n",
    "print(\"\\nPerforming enhanced sentiment analysis...\")\n",
    "sentiment_features = df['clean_description'].apply(detailed_sentiment_analysis)\n",
    "df = pd.concat([df, pd.DataFrame(sentiment_features.tolist())], axis=1)\n",
    "\n",
    "# ======================\n",
    "# NAMED ENTITY TRACKING\n",
    "# ======================\n",
    "def track_entity_trends(df, window_days=30):\n",
    "    \"\"\"Track entity popularity over time\"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    entity_trends = []\n",
    "    \n",
    "    for end_date in df['create_time'].unique():\n",
    "        start_date = end_date - pd.Timedelta(days=window_days)\n",
    "        window_data = df[(df['create_time'] >= start_date) & (df['create_time'] <= end_date)]\n",
    "        \n",
    "        entities = []\n",
    "        for text in window_data['clean_description']:\n",
    "            doc = nlp(text)\n",
    "            entities.extend([ent.text.lower() for ent in doc.ents])\n",
    "        \n",
    "        if entities:\n",
    "            entity_counts = Counter(entities)\n",
    "            entity_trends.append({\n",
    "                'date': end_date,\n",
    "                'top_entities': dict(entity_counts.most_common(10)),\n",
    "                'total_mentions': len(entities)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(entity_trends)\n",
    "\n",
    "print(\"\\nTracking entity trends...\")\n",
    "entity_trends = track_entity_trends(df)\n",
    "\n",
    "# ======================\n",
    "# VISUALIZATION & REPORTING\n",
    "# ======================\n",
    "\n",
    "# 1. Topic Evolution Plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "for topic_idx in range(N_TOPICS):\n",
    "    plt.plot(topic_evolution['date'], \n",
    "             topic_evolution['top_topics'].apply(lambda x: x[topic_idx]),\n",
    "             label=f'Topic {topic_idx+1}')\n",
    "plt.title('Topic Evolution Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Topic Prominence')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 2. Sentiment Analysis Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(data=df, x='polarity', y='virality_score', \n",
    "                hue='is_trending', alpha=0.5)\n",
    "plt.title('Sentiment Polarity vs Virality Score')\n",
    "plt.show()\n",
    "\n",
    "# 3. Entity Trend Analysis\n",
    "plt.figure(figsize=(15, 6))\n",
    "entity_trends.set_index('date')['total_mentions'].plot()\n",
    "plt.title('Entity Mention Volume Over Time')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "output_path = 'data/processed/nlp_trends_analysis.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"\\nAnalysis complete. Results saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiktok_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
